{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisites\n",
    "\n",
    "### 1. Load Spacy and Download Language Model from Spacy\n",
    "- pip install spacy\n",
    "- python -m spacy download en_core_web_md\n",
    "\n",
    "### 2. Initialise Youtube API\n",
    "- https://developers.google.com/youtube/v3/quickstart/python\n",
    "- Use this key in your application by passing it with the key=API_KEY parameter. AIzaSyBFB9_Plcj-N7tpF2p08IJngcOwNQFnvrI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "import distance\n",
    "from fuzzywuzzy import fuzz\n",
    "import statistics\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "API_KEY = 'AIzaSyCYhjAqZWZUkMDI4gd3rkspFHEoXX7KDi4'\n",
    "nlp = spacy.load(\"en_core_web_md\")  \n",
    "youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text.split() if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab - english_vocab\n",
    "    return sorted(unusual)\n",
    "\n",
    "def get_oov_words(query):\n",
    "    doc = nlp(query)\n",
    "    oov_words = [word.text for word in doc if word.is_oov]\n",
    "    oov_words_string = ' '.join(word.lower() for word in list(oov_words))\n",
    "    return oov_words_string\n",
    "\n",
    "def get_oov_words2(query):\n",
    "    query=query.lower()\n",
    "    query_filtered = unusual_words(query)\n",
    "    oov_words = [word for word in query_filtered if not wordnet.synsets(word)]\n",
    "    oov_words_string = ' '.join(word.lower() for word in list(oov_words))\n",
    "    return oov_words_string\n",
    "\n",
    "def filter_query(query):\n",
    "    doc = nlp(query)\n",
    "    filtered_words = [word.text for word in doc if not word.is_oov]\n",
    "    filtered_words_string = ' '.join(word.lower() for word in list(filtered_words))\n",
    "    return filtered_words_string\n",
    "\n",
    "def clean_up_texts(text):\n",
    "    text = re.sub(r'https\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n",
    "    text = re.sub(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\\S]*\\s?','',text)\n",
    "    text = ''.join(e for e in text if e.isalpha() or e.isspace())\n",
    "   \n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def get_videos(query, number_of_videos):\n",
    "    \n",
    "\n",
    "    oov_words_in_search_query = get_oov_words(query)\n",
    "    filtered_query = filter_query(query)\n",
    "\n",
    "    \n",
    "    nextPageToken = None\n",
    "    allVideos = []\n",
    "    _counter = number_of_videos\n",
    "    videoId = []\n",
    "    title = []\n",
    "    description = []\n",
    "    oov_words = []\n",
    "    \n",
    "    while True:\n",
    "        if _counter < 50:\n",
    "            MAX_COUNT = _counter\n",
    "        else:\n",
    "            MAX_COUNT = 50\n",
    "\n",
    "        req = youtube.search().list(q=filtered_query, part='snippet', type='video', maxResults=MAX_COUNT, pageToken=None)\n",
    "        res = req.execute()\n",
    "        _counter = _counter-50\n",
    "        nextPageToken = res['nextPageToken']\n",
    "        items = res['items']\n",
    "        for each_item in items:\n",
    "            allVideos += each_item\n",
    "            \n",
    "            _videoId = each_item['id']['videoId']\n",
    "            _title = each_item['snippet']['title']\n",
    "            _description = each_item['snippet']['description']\n",
    "            \n",
    "            _title = clean_up_texts(_title)\n",
    "            _description = clean_up_texts(_description)\n",
    "            _oov_words = get_oov_words2(_title) +' '+ get_oov_words2(_description)\n",
    "\n",
    "            videoId.append(_videoId)\n",
    "            title.append(_title)\n",
    "            description.append(_description)\n",
    "            oov_words.append(_oov_words)\n",
    "        if res['nextPageToken'] == None:\n",
    "            break;  # exit from the loop\n",
    "        if _counter <=0 :\n",
    "            break;  # exit from the loop\n",
    "\n",
    "    data = {'videoId': videoId,'title': title, 'description': description, 'oov_words':oov_words\n",
    "            ,'oov_lookup':oov_words_in_search_query }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'raghab hindi Valentine Cover'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raghab'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_oov_words2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>oov_words</th>\n",
       "      <th>oov_lookup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vFN3eNe0_Hs</td>\n",
       "      <td>pehla nasha valentines day special  sanam</td>\n",
       "      <td>this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha</td>\n",
       "      <td>nasha pehla sanam nasha phela sanam sanamrendition</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RyRgdKGSctU</td>\n",
       "      <td>valentines medley  na tum jaano na hum  pee lu  tum jo aaye  mast magan  raghav chaitanya</td>\n",
       "      <td>subscribe to my youtube channel   hi guys valentines day is special and on this special day here i am with my medley of</td>\n",
       "      <td>aaye chaitanya jaano magan raghav youtube</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NbpdEetp6Hk</td>\n",
       "      <td>valentines medley    raghav chaitanya</td>\n",
       "      <td>subscribe to my youtube channel   hi im back with my valentines medley for the year  happy valentines day to everyone</td>\n",
       "      <td>chaitanya raghav im youtube</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>j45TDsUSEVk</td>\n",
       "      <td>bodo and hindi mashup cover song  happy valentines day to all my loving friends   feb</td>\n",
       "      <td>music  videography  premanto narzary</td>\n",
       "      <td>mashup narzary premanto videography</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tIv-tO8Ba-0</td>\n",
       "      <td>valentines day special   bahut pyaar karte hain  debolinaa nandy  ft badal s  cover</td>\n",
       "      <td>love never dies love will continue love keeps on beating when youre gone love never dies once it is in you life may be fleeting love lives on soo my</td>\n",
       "      <td>badal debolinaa karte nandy pyaar soo youre</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId  \\\n",
       "0  vFN3eNe0_Hs   \n",
       "1  RyRgdKGSctU   \n",
       "2  NbpdEetp6Hk   \n",
       "3  j45TDsUSEVk   \n",
       "4  tIv-tO8Ba-0   \n",
       "\n",
       "                                                                                       title  \\\n",
       "0  pehla nasha valentines day special  sanam                                                   \n",
       "1  valentines medley  na tum jaano na hum  pee lu  tum jo aaye  mast magan  raghav chaitanya   \n",
       "2  valentines medley    raghav chaitanya                                                       \n",
       "3  bodo and hindi mashup cover song  happy valentines day to all my loving friends   feb       \n",
       "4  valentines day special   bahut pyaar karte hain  debolinaa nandy  ft badal s  cover         \n",
       "\n",
       "                                                                                                                                               description  \\\n",
       "0  this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha    \n",
       "1  subscribe to my youtube channel   hi guys valentines day is special and on this special day here i am with my medley of                                   \n",
       "2  subscribe to my youtube channel   hi im back with my valentines medley for the year  happy valentines day to everyone                                     \n",
       "3  music  videography  premanto narzary                                                                                                                      \n",
       "4  love never dies love will continue love keeps on beating when youre gone love never dies once it is in you life may be fleeting love lives on soo my      \n",
       "\n",
       "                                            oov_words oov_lookup  \n",
       "0  nasha pehla sanam nasha phela sanam sanamrendition  raghab     \n",
       "1  aaye chaitanya jaano magan raghav youtube           raghab     \n",
       "2  chaitanya raghav im youtube                         raghab     \n",
       "3  mashup narzary premanto videography                 raghab     \n",
       "4  badal debolinaa karte nandy pyaar soo youre         raghab     "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_videos(query,10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# return list from series of comma-separated strings\n",
    "def chainer(s):\n",
    "    return list(chain.from_iterable(s.str.split(' ')))\n",
    "\n",
    "# calculate lengths of splits\n",
    "lens = df['oov_words'].str.split(' ').map(len)\n",
    "\n",
    "# create new dataframe, repeating or chaining as appropriate\n",
    "res = pd.DataFrame({'videoId': np.repeat(df['videoId'], lens),\n",
    "                    'title': np.repeat(df['title'], lens),\n",
    "                    'description': np.repeat(df['description'], lens),\n",
    "                    'oov_words': chainer(df['oov_words']),\n",
    "                   'oov_lookup': np.repeat(df['oov_lookup'], lens)})\n",
    "df  = res.reset_index(drop=True)\n",
    "\n",
    "df['oov_words'].replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=['oov_words'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>oov_words</th>\n",
       "      <th>oov_lookup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vFN3eNe0_Hs</td>\n",
       "      <td>pehla nasha valentines day special  sanam</td>\n",
       "      <td>this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha</td>\n",
       "      <td>nasha</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vFN3eNe0_Hs</td>\n",
       "      <td>pehla nasha valentines day special  sanam</td>\n",
       "      <td>this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha</td>\n",
       "      <td>pehla</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vFN3eNe0_Hs</td>\n",
       "      <td>pehla nasha valentines day special  sanam</td>\n",
       "      <td>this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha</td>\n",
       "      <td>sanam</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vFN3eNe0_Hs</td>\n",
       "      <td>pehla nasha valentines day special  sanam</td>\n",
       "      <td>this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha</td>\n",
       "      <td>nasha</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vFN3eNe0_Hs</td>\n",
       "      <td>pehla nasha valentines day special  sanam</td>\n",
       "      <td>this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha</td>\n",
       "      <td>phela</td>\n",
       "      <td>raghab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       videoId                                      title  \\\n",
       "0  vFN3eNe0_Hs  pehla nasha valentines day special  sanam   \n",
       "1  vFN3eNe0_Hs  pehla nasha valentines day special  sanam   \n",
       "2  vFN3eNe0_Hs  pehla nasha valentines day special  sanam   \n",
       "3  vFN3eNe0_Hs  pehla nasha valentines day special  sanam   \n",
       "4  vFN3eNe0_Hs  pehla nasha valentines day special  sanam   \n",
       "\n",
       "                                                                                                                                               description  \\\n",
       "0  this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha    \n",
       "1  this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha    \n",
       "2  this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha    \n",
       "3  this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha    \n",
       "4  this valentines day come fall in love again with your favorite band sanam as they present yet another sanamrendition of the super hit song phela nasha    \n",
       "\n",
       "  oov_words oov_lookup  \n",
       "0  nasha     raghab     \n",
       "1  pehla     raghab     \n",
       "2  sanam     raghab     \n",
       "3  nasha     raghab     \n",
       "4  phela     raghab     "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "_array = np.asarray([words for segments in df.oov_words for words in segments.split()]) \n",
    "words = np.insert( _array, len(_array), get_oov_words(query) )\n",
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, -4, -4, ..., -6, -7, -3],\n",
       "       [-4,  0, -5, ..., -7, -7, -5],\n",
       "       [-4, -5,  0, ..., -6, -7, -4],\n",
       "       ...,\n",
       "       [-6, -7, -6, ...,  0, -7, -6],\n",
       "       [-7, -7, -7, ..., -7,  0, -7],\n",
       "       [-3, -5, -4, ..., -6, -7,  0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3  Using K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import fasttext\n",
    "ft_model = fasttext.load_model('wiki.simple/wiki.simple.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "    return np.dot(vec_a, vec_b) / \\\n",
    "        (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity = -1*np.array([[cosine_similarity(\n",
    "    ft_model.get_word_vector(w1),ft_model.get_word_vector(w2))\n",
    "                               for w1 in words] for w2 in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        , -0.4142262 , -0.36530843, ..., -0.11352272,\n",
       "        -0.29796284, -0.4405388 ],\n",
       "       [-0.4142262 , -1.        , -0.42266458, ..., -0.23738801,\n",
       "        -0.424228  , -0.47339624],\n",
       "       [-0.36530843, -0.42266458, -0.9999999 , ..., -0.15476146,\n",
       "        -0.3166491 , -0.40475303],\n",
       "       ...,\n",
       "       [-0.11352272, -0.23738801, -0.15476146, ..., -1.        ,\n",
       "        -0.25787985, -0.20113617],\n",
       "       [-0.29796284, -0.424228  , -0.3166491 , ..., -0.25787985,\n",
       "        -1.        , -0.35617706],\n",
       "       [-0.4405388 , -0.47339624, -0.40475303, ..., -0.20113617,\n",
       "        -0.35617706, -0.99999994]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=25, n_init=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 25\n",
    "kmeans_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "kmeans_model.fit(cos_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m0\u001b[0m: aspl\n",
      "\u001b[1m1\u001b[0m: mulakat, singhthatsings\n",
      "\u001b[1m2\u001b[0m: banita, magan, phela\n",
      "\u001b[1m3\u001b[0m: youtube\n",
      "\u001b[1m4\u001b[0m: mashup\n",
      "\u001b[1m5 raghab, raghav\u001b[0m:\n",
      "\u001b[1m6\u001b[0m: pyaar\n",
      "\u001b[1m7\u001b[0m: nasha\n",
      "\u001b[1m8\u001b[0m: chaitanya, jaitay\n",
      "\u001b[1m9\u001b[0m: datelove, debolinaa, premanto\n",
      "\u001b[1m10\u001b[0m: ajnabee, badal, haseena, jaano, pehla\n",
      "\u001b[1m11\u001b[0m: im\n",
      "\u001b[1m12\u001b[0m: curated\n",
      "\u001b[1m13\u001b[0m: tseries\n",
      "\u001b[1m14\u001b[0m: sanamrendition\n",
      "\u001b[1m15\u001b[0m: kabir, narzary\n",
      "\u001b[1m16\u001b[0m: aaye, youre\n",
      "\u001b[1m17\u001b[0m: jangir\n",
      "\u001b[1m18\u001b[0m: videography\n",
      "\u001b[1m19\u001b[0m: ek\n",
      "\u001b[1m20\u001b[0m: gai, soo\n",
      "\u001b[1m21\u001b[0m: sanam\n",
      "\u001b[1m22\u001b[0m: manchanda, nandy\n",
      "\u001b[1m23\u001b[0m: adi\n",
      "\u001b[1m24\u001b[0m: karte\n"
     ]
    }
   ],
   "source": [
    "model_score=[]\n",
    "for cluster_id in np.unique(kmeans_model.labels_):\n",
    "    exemplar = cluster_id\n",
    "    cluster = np.unique(words[np.nonzero(kmeans_model.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster) \n",
    "    \n",
    "    oov_lookup = get_oov_words(query)\n",
    "    if oov_lookup in cluster_str:\n",
    "        print(f\"\\033[1m{exemplar} {cluster_str }\\033[0m:\" )\n",
    "        for cluster_word in cluster_str.split():\n",
    "            if oov_lookup!= cluster_word:\n",
    "                model_score.append(fuzz.ratio(oov_lookup, cluster_word))\n",
    "    else:\n",
    "        print(f\"\\033[1m{exemplar}\\033[0m: {cluster_str }\" )\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(model_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
